---
layout: default
title: 日志查询工具介绍
---
作为一个服务端开发，线上进行日志查询是家常便饭。而要查询线上日志，一般意味着是线上问题，早点定位到问题，就会使损失少一点，掌握一些工具使用技巧将会使你更快的通过日志定位到问题。首先声明本文目的并不是想成为一片介绍指令使用方法的文章，而是希望当你也遇到类似应用场景的话，直接从本文中复制指令，稍加修改就可以使用的文章。另外我的硬件是：macOS+iterm2，mac配置如下图所示:

![GitHub](http://dbp-resource.cdn.bcebos.com/a1620f93-4200-9024-4be8-61a6751b1340/Lark20210106104630.png "GitHub,Social Coding")

可能是自己所做业务的关系，上下游之间通过一个独一无二的requestId来标识同一个请求，因此我最常用的是这个命令：

     grep -irn 'c066ff578b784efe8e1d64c73df8880a' application.log

因为一个业务系统对一个请求会进行各种处理，每次处理都会打印日志，采用这种方式会把所有处理的日志记录全部打印出来，量多的时候好几屏都刷不完。如果你对自己的系统比较熟悉，会首先猜想下出问题的大概地方，然后查看对应地方的日志来确认问题，我个人一般会记录下对应日志的打印类来达到这种目的，如下所示：

     grep -irn 'c066ff578b784efe8e1d64c73df8880a' application.log | grep -i 'skillrankeractor'

这个地方主要想强调两点：1、grep查找匹配可以忽略大小写，使用提示符-i即可；2、grep支持正则表达式，可以给你提供更丰富的查询手段。由于很多时候我们不能完全清楚的记得类的具体名字，大小写等信息。因此合理地使用这两点可以有效地提高效率。对于正则表达式，又分为基本正则表达式和扩展正则表达式，基本正则表达式grep默认是支持的，如果是扩展表达式，则需要加上-E。假如你分不清楚是基本还是扩展，统一加上-E即可。我本人是这样使用的：

    grep -irn 'c066ff578b784efe8e1d64c73df8880a' application.log | grep -i 'postprocessing.*actor'
    grep -irn 'c066ff578b784efe8e1d64c73df8880a' application.log | grep -i 'postprocessing[0-9a-zA-Z]{2}actor’
    
再来看一个更强大一点的：
    
    //统计20:15~20:25 的 包含了 Server side error Status 的日志记录数，后面awk会更加强大
    grep -irn 'com.xiaomi.common.service.ServerRetryableException: Server side error Status(500,head of empty list' -B 1 application-2021-01-04.0.err.log | grep -E '(2021-01-04 20:2[0-5])|(2021-01-04 20:1[6-9])'  | wc -l

grep 只能是简单的匹配日志，如果匹配的上就打印出来，如果匹配不上就不打印，逻辑功能比较简单。如果你有一些复杂的需求，grep可能就无法满足，你就需要其他工具来配合你使用了。我本人遇到的一种场景如下：某一天有人提醒你服务异常数过多，而这个异常多个下游业务方都有可能打印，因此你需要尽快定位到是哪个业务方并提醒他们，这是使用grep就力不从心了。此时我会选择另外一个工具：awk。grep能干的awk都可以，awk能干的grep却不一定可以。
        
关于awk本文只想重点表达两点：1、awk如何定义分隔符，可以通过-F来实现，因为不同的分割符对你如何处理会有影响；2、awk的简单应用。首先默认的分隔符是\tab，你可以通过-F来指定其它的分割符，注意可以同时指定多个，如下所示：

    awk -F ' |,' '{print $4}' application-2021-01-04.0.err.log | head -n 10
    
awk用法很多，讲解它的书的厚度和《Mysql权威指南》差不多。此处只讲一些最简单的，它的基本格式如下所示：

    //说明一下，动作一是必须的，动作二是可选的，是在整个文件遍历完成了才会执行。
    awk '条件 {动作1} END {动作2}' 文件名  

结合我刚才所说的场景，首先你需要熟悉你的日志结构，定义良好的分割符，并打印分割结果，这样你就知道应该根据第几个字段来统计分析你的日志了，我常用的操作是：
    
    awk -F ' |,' '{for (i=1;i<NF;i++) print i,$i}' application-2021-01-04.0.err.log | head -n 10

如下图所示：
![GitHub](http://dbp-resource.cdn.bcebos.com/a1620f93-4200-9024-4be8-61a6751b1340/%E9%80%89%E5%8F%96%E5%88%97.png "GitHub,Social Coding")

一旦你确认好分隔符，以及根据哪一个列来统计分析，接下来你可以可以领略一下awk的威力了，如下所示：

    //查询20:15:00~20:26:00，所有包含了Call to.* failed的日志记录的条数
    awk -F ' |,' '/Call to.* failed/  && $3>"20:15:00" && $3<"20:26:00"' application-2021-01-04.0.err.log | wc -l 
    
    //查询20:15:00~20:26:00，所有包含了Call to.* failed的日志记录的条数，按照第11列字段来进行统计，没出现一次就+1，并从大到小排列
    awk -F ' |,' '/Call to.* failed/  && $3>"20:15:00" && $3<"20:26:00"' application-2021-01-04.0.err.log | awk -F ' |,' '{a[$11]++} END {for(i in a) {print i, a[i]}}' | sort -nk 2

如图所示：
![GitHub](http://dbp-resource.cdn.bcebos.com/a1620f93-4200-9024-4be8-61a6751b1340/Lark20210105205322.png "GitHub,Social Coding")

讲解到这儿，你应该也注意到了，无论是grep还是awk只能查询所在机器的日志。要是机器多，一个个就会效率太低。这里再推荐一个工具，见：[polysh使用教程](https://www.cnblogs.com/butterfly100/p/9015531.html)，这里只想强调一点，现在生产环境都是禁止线下机器直接连接的，必要通过跳板机，因此要想正常使用需要配置对安装了polysh机器来免密登录。

最后补充一下，本人一直用的是iterm2，有一个快捷操作比如删除当前行或者当前单词，非常管用，具体可以见文章：[iterm2快捷操作](https://cnbin.github.io/blog/2015/06/20/iterm2-kuai-jie-jian-da-quan/)



