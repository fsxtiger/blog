---
layout: default
title: 从一次海量数据刷新开始
---
笔者在项目实践中，遇到这样一个场景：项目在升级部署的时候，某张表中几乎全部数据需要更新。由于更新过程需要在正常之前完成，因此对效率有一定的要求。在测试环境，数据存储在mongo中，总量为3600W。最初的处理方案，没有考虑到数据量过大的场景，常规地将表数据全部读入内存，然后进行处理，最后批量更新。测试过程发现服务报 FullGC, 于是笔者开始介入这部分功能的性能提升。首先自嘲一下，3600W数据算不上海量，笔者将本文题目命名为海量数据，是为了提醒自己，在进行方案设计时，要时刻想到当数据量非常大时，设计方案是否可以灵活扩展。  

FullGC的问题比较好排查，分析内存文件, 发现业务对象WarnInfo过多。将之前的全量读取方式改为流式读取，FullGC问题就解决了，内存分析如下所示：
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/screenshot-1.png" height = "200" width="100%"/>

重新测试发现，全部数据处理完毕，大约花费一个小时左右，无法满足产品要求。笔者首先想到的是分页多线程并行查询。先统计需要更新的条目总数，确定好每页数量(limit)，计算出需要多少页以及每页的offset，将每页都当做一个Task提交线程池。经过多次测试发现，每页limit越大，总耗时越小，统计结果如下所示：  

| limit(条)   | 50W       | 10W        | 1W      | 
| :--- | :--- | :--- | :--- |
| 时间(ms)    | 498,551     | 971,865    | 1,566,678    | 

笔者推测，每页读取的数量越少，需要读取的任务数就越多，而需要读取数据总数是一定的，说明耗时的地方可能是找到每页偏移量offset的位置。为了验证推测，笔者查看mongo的慢查询日志，其中单次查询如下所示：  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/query1.jpg" height = "450" width="100%"/>

由图可见，一次流式查询是由一次find 和 若干次getMore操作。第一次find操作，返回了101个结果(nreturned值，不设置默认为101)，以及下次开始查询的游标（cursorid）。可以看到第一次find耗时2048ms，其它的getMore操作都是200ms左右，每次读取数在26000左右。由此可以证实推测，每次读取的量越少，需要读取的次数就越频繁，需要定位到offset的操作就越多，而这个操作是最耗时的，因此总耗时也会越多。笔者继续查看不同页读取耗时，如下所示：  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/query2.jpg" height = "450" width="100%"/>  

由图可见，随着偏移量逐渐增大，耗时在逐渐增加。offset为50W时，耗时2048ms，到了300W时，耗时为13940ms，这属于典型的深分页问题。当偏移量过大，可能会导致超时。因此本方案在海量数据的场景中存在问题，不具备扩展性。深分页问题是一个比较普遍问题，和使用哪种存储器无关，无论是mysql，还是mongo。解决办法在网上也很多，总的来说分成两类：
* 每次查询需要返回上次的id（或者其它索引列），使用${id} > id 来取代skip操作
* 使用between……and，也是通过索引列，来取代skip操作。这个需要提前计算好区间

笔者本次就是结合两种方式。使用id来对数据进行分区，在每个区间内使用between……and来进行检索。id的选择，笔者使用mongo表的_id字段。每个mongo表都有这个字段，保证其唯一性并创建主键索引，所以方案与具体业务无关。_id的默认生成过程类似于雪花算法，是24位的16进制数，绝大部分情况下可以保证有序和均匀分布，因此可以确保在每个区间的数据量相对均衡。这是相比采用其他业务字段所具有的优势。由于要进行任务划分，笔者无疑要选择Fork/Join线程池，处理流程如下：  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" height = "500" width="450"/>

再次查看每次查询的耗时情况：  

| limit(条) | 1W | 5000 | 1000 |
| :--- |:---- | :---- | :----  |
| 时间(ms) | 845,445 | 444,468 |  658,836 | 

可以发现每次查询的越少，总体耗时反而越少。笔者推测，由于调整了逻辑，游标定位的时间变少。单次查询条数变少，线程之间负载越均衡，Fork/Join对于大量小耗时任务是比较友好的，可以从慢查询日志中看到，游标定位的时间已经回归正常了:  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/query3.jpg" height = "450" width="100%"/>  

可以发现每次find定位游标的时间都是100ms的水平，而且分布均匀。虽然耗时较长的问题已有明显缓解，但是笔者在查看慢查询日志时，发现有如下慢查询：
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/%E4%B8%BB%E9%94%AE.png" height = "340" width="100%"/>

这条查询已经走了_id主键索引，并且限制limit=1，却检查了全部索引和文档(documents)。笔者推测，符合筛选条件{"dstType":{$in:[2,3]}}的个数为0，因此每遍历一个_id，然后根据_id来获取document，遍历完毕都没有找到一个（limit = 1）符合条件的，变相触发全表扫描。通过查询mongo，确认的确没有符合筛选条件的。这种场景可以通过建立联合索引{_id:1, dstType:1}来优化。继续观察，还有其他的慢查询：  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/find.png" height = "340" width="100%"/>

一个是find操作，一个是count操作。检查的索引(Key)数和检查的文档数一致。count操作是因为只走了_id索引，需要通过_id来加载document检查srcType是否符合条件所致，同样可以通过添加联合索引解决。find操作是因为代码指明要获取所有字段引起回表查询导致，其实只需要srcIp。通过添加联合索引{_id:1, srcType:1, srcIp:1} 和 {_id:1, dstType:1, dstIp:1} ，由图可见docsExamined均变为0，整体耗时又下降2min左右：  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/query2.png" height = "340" width="100%"/>

虽然耗时较长的问题已有缓解，但在测试验证发现，在刷数据过程中cpu会飙升至100%。由于此时项目已经启动完成并对外提供服务，cpu使用率到100%会影响用户操作，需要进一步调整。按照常规手段，笔者首先查看了消耗CPU最多的进程，如下所示：  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/cpu.png" height = "340" width="100%"/>

可见消耗cpu最多的进程是mongod，然后是java。笔者发现java进程和mongod 进程在一台主机上。之前笔者根据经验，鉴于读写mongo是IO密集型的，因此核心线程数设置为主机核数的两倍，现在想想其实隐藏了一个前提，这个服务是单机部署的。笔者将核心线程数调整为核数的3/4，重新测试，CPU使用率在70%~80%，总耗时无明显变化，再次在实践中证明，线程数并非越多越好，线程数过多导致CPU消耗过多，线程调度消耗增加，可能会有损整体耗时。  
<img src="http://dbp-resource.cdn.bcebos.com/9abe0e71-8c18-ab1c-22ac-a4f210feae48/cpu.jpeg" height = "340" width="100%"/>

在这次调优中，笔者通过合理分区解决深分页问题，通过查阅慢查询日志建立索引优化查询效率问题，通过调整Fork/Join线程池参数解决CPU飙升问题，最终3600W数据大约五分钟内全部刷新完毕，cpu使用率在70%~80%之间。最终代码经过笔者几次修正如下所示：
```
    private abstract class RefreshAction extends RecursiveAction {
        private String startId;
        private String endId;
        private int limit = 5000; //单次查询个数不能超过limit，否则继续进行拆分

        public RefreshAction(String startId, String endId) {
            this.startId = startId;
            this.endId = endId;
        }

        @Override
        protected void compute() {
            long count = count(startId, endId);

            if (count <= limit) {
                doHandle(startId, endId);
                return;
            }
            for (Pair<String, String> pair : splitRage(count)) {
                new RefreshAction(pair.getFirst(), pair.getSecond()) {
                    @Override
                    void doHandle(String startId, String endId) {
                        RefreshAction.this.doHandle(startId, endId);
                    }

                    @Override
                    long count(String startId, String endId) {
                        return RefreshAction.this.count(startId, endId);
                    }
                }.fork();
            }
        }

        private List<Pair<String, String>> splitRage(long count) {
            long size = (count + limit - 1) / limit;
            BigInteger pageSize = new BigInteger(Long.toString(size));
            BigInteger start = new BigInteger(startId, 16);
            BigInteger end = new BigInteger(endId, 16);
            BigInteger step = ((end.subtract(start)).divide(pageSize)).add(one);

            List<Pair<String, String>> pairs = new ArrayList<>();
            for (int x = 0; x < size; x ++, start = start.add(step)) {
                Pair<String, String> pair = Pair.of(start.toString(16), start.add(step).toString(16));
                pairs.add(pair);
            }
            return pairs;
        }

        abstract void doHandle(String startId, String endId);

        abstract long count(String startId, String endId);
    }
    
    private void refreshAccessRelationAndBlocking() {
        ForkJoinPool forkJoinPool = new ForkJoinPool(Runtime.getRuntime().availableProcessors() * 3 / 4);
        long startAt = System.currentTimeMillis();
        refreshWarnInfo(forkJoinPool);
        refreshAccessBlocking(forkJoinPool);
        refreshAccessRelation(forkJoinPool);
        try {
            forkJoinPool.shutdown();
            forkJoinPool.awaitTermination(1, TimeUnit.HOURS);
        } catch (InterruptedException e) {
            LOGGER.error("refresh failed");
        }
        LOGGER.info("refreshAccessRelationAndBlocking cost = " + (System.currentTimeMillis() - startAt));
    }
```






